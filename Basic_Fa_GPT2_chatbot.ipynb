{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/MaryamNourii/ChatBot/blob/main/Basic_Fa_GPT2_chatbot.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-_jjlVOK8pmW",
        "outputId": "1216f710-6851-4b61-db68-4fc58f4f09da"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: tensorflow in /usr/local/lib/python3.10/dist-packages (2.12.0)\n",
            "Requirement already satisfied: absl-py>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.4.0)\n",
            "Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.6.3)\n",
            "Requirement already satisfied: flatbuffers>=2.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (23.3.3)\n",
            "Requirement already satisfied: gast<=0.4.0,>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (0.4.0)\n",
            "Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (0.2.0)\n",
            "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.54.0)\n",
            "Requirement already satisfied: h5py>=2.9.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (3.8.0)\n",
            "Requirement already satisfied: jax>=0.3.15 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (0.4.10)\n",
            "Requirement already satisfied: keras<2.13,>=2.12.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (2.12.0)\n",
            "Requirement already satisfied: libclang>=13.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (16.0.0)\n",
            "Requirement already satisfied: numpy<1.24,>=1.22 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.22.4)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (3.3.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from tensorflow) (23.1)\n",
            "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (3.20.3)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from tensorflow) (67.7.2)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.16.0)\n",
            "Requirement already satisfied: tensorboard<2.13,>=2.12 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (2.12.2)\n",
            "Requirement already satisfied: tensorflow-estimator<2.13,>=2.12.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (2.12.0)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (2.3.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.6 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (4.5.0)\n",
            "Requirement already satisfied: wrapt<1.15,>=1.11.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.14.1)\n",
            "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (0.32.0)\n",
            "Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from astunparse>=1.6.0->tensorflow) (0.40.0)\n",
            "Requirement already satisfied: ml-dtypes>=0.1.0 in /usr/local/lib/python3.10/dist-packages (from jax>=0.3.15->tensorflow) (0.1.0)\n",
            "Requirement already satisfied: scipy>=1.7 in /usr/local/lib/python3.10/dist-packages (from jax>=0.3.15->tensorflow) (1.10.1)\n",
            "Requirement already satisfied: google-auth<3,>=1.6.3 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.13,>=2.12->tensorflow) (2.17.3)\n",
            "Requirement already satisfied: google-auth-oauthlib<1.1,>=0.5 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.13,>=2.12->tensorflow) (1.0.0)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.13,>=2.12->tensorflow) (3.4.3)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.13,>=2.12->tensorflow) (2.27.1)\n",
            "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.13,>=2.12->tensorflow) (0.7.0)\n",
            "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.13,>=2.12->tensorflow) (1.8.1)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.13,>=2.12->tensorflow) (2.3.0)\n",
            "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.13,>=2.12->tensorflow) (5.3.0)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.13,>=2.12->tensorflow) (0.3.0)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.13,>=2.12->tensorflow) (4.9)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from google-auth-oauthlib<1.1,>=0.5->tensorboard<2.13,>=2.12->tensorflow) (1.3.1)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.13,>=2.12->tensorflow) (1.26.15)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.13,>=2.12->tensorflow) (2022.12.7)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.13,>=2.12->tensorflow) (2.0.12)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.13,>=2.12->tensorflow) (3.4)\n",
            "Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.10/dist-packages (from werkzeug>=1.0.1->tensorboard<2.13,>=2.12->tensorflow) (2.1.2)\n",
            "Requirement already satisfied: pyasn1<0.6.0,>=0.4.6 in /usr/local/lib/python3.10/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard<2.13,>=2.12->tensorflow) (0.5.0)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.10/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<1.1,>=0.5->tensorboard<2.13,>=2.12->tensorflow) (3.2.2)\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting transformers\n",
            "  Downloading transformers-4.30.2-py3-none-any.whl (7.2 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.2/7.2 MB\u001b[0m \u001b[31m54.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.12.0)\n",
            "Collecting huggingface-hub<1.0,>=0.14.1 (from transformers)\n",
            "  Downloading huggingface_hub-0.15.1-py3-none-any.whl (236 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m236.8/236.8 kB\u001b[0m \u001b[31m24.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.22.4)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (23.1)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2022.10.31)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.27.1)\n",
            "Collecting tokenizers!=0.11.3,<0.14,>=0.11.1 (from transformers)\n",
            "  Downloading tokenizers-0.13.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (7.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.8/7.8 MB\u001b[0m \u001b[31m81.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting safetensors>=0.3.1 (from transformers)\n",
            "  Downloading safetensors-0.3.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m62.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.65.0)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.14.1->transformers) (2023.4.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.14.1->transformers) (4.5.0)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (1.26.15)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2022.12.7)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.0.12)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.4)\n",
            "Installing collected packages: tokenizers, safetensors, huggingface-hub, transformers\n",
            "Successfully installed huggingface-hub-0.15.1 safetensors-0.3.1 tokenizers-0.13.3 transformers-4.30.2\n"
          ]
        }
      ],
      "source": [
        "!pip install tensorflow\n",
        "!pip install transformers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1_8tfQ758kgw"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "from transformers import AutoTokenizer, TFAutoModelForCausalLM\n",
        "\n",
        "model_name = \"HooshvareLab/gpt2-fa\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "model = TFAutoModelForCausalLM.from_pretrained(model_name)\n",
        "\n",
        "def generate_response(input_text, max_length=500):\n",
        "    encoded_input = tokenizer(input_text, return_tensors='tf')\n",
        "\n",
        "    generated_output = model.generate(\n",
        "        input_ids=encoded_input['input_ids'],\n",
        "        attention_mask=encoded_input['attention_mask'],\n",
        "        max_length=max_length,\n",
        "        num_beams=5,\n",
        "        no_repeat_ngram_size=2,\n",
        "        early_stopping=True\n",
        "    )\n",
        "\n",
        "    generated_text = tokenizer.decode(generated_output[0], skip_special_tokens=True)\n",
        "\n",
        "    return generated_text.strip()\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "usySoAPj9wCp"
      },
      "outputs": [],
      "source": [
        "# input_text = 'سلام چطوری؟ من نمی دونم چیکار کنم با این درسا، لطفا زاهنماییم کن'\n",
        "input_text = 'سلام چطوری؟ من چیکار کنم؟'\n",
        "\n",
        "resp = generate_response(input_text)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KmGyK5gY-orZ",
        "outputId": "694dbe5b-f697-4148-e64e-3834e6afbacb"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Generated response: سلام چطوری؟ من نمی دونم چیکار کنم با این درسا، لطفا زاهنماییم کنارتون رو عوض کنید. خب حالا بریم سراغ اصل مطلب. اول از همه باید بگم که من به عنوان یک برنامه نویس حرفه‌ای، همیشه سعی می‌کنم خودم رو به روز نگه دارم و همیشه در حال یادگیری باشم. به همین دلیل هم هست که خیلی از برنامه نویسان تازه کار، به دنبال یادگیری مهارت‌های برنامه نویسی هستند. در این مقاله قصد دارم به شما یاد بدم که چطور با استفاده از زبان‌ها و فریم ورک هایی مثل React، Angular، Vue، React و … به راحتی و با صرف کمترین زمان، برنامه‌ی خود را بسازید. پس با من همراه باشید. برای شروع، شما نیاز دارید که یک وب سایت یا یک اپلیکیشن را ایجاد کنید و سپس آن را در مرورگر خود باز کرده و شروع به ساخت یک وبسایت یا اپلیکیشن جدید بکنید. این کار بسیار ساده است، فقط کافی است که به سایت خود بروید و در آن ثبت نام کنید، سپس یک حساب کاربری بسازید و وارد آن شوید. بعد از ساخت وبسایت، باید به سراغ ساخت اپلیکیشن بروید. شما باید یک اکانت بسازید تا بتوانید از طریق آن، اپلیکیشن یا وب سایتی که ساخته‌اید را به صورت رایگان در اختیار دیگران قرار دهید. اگر شما هم از آن دسته افرادی هستید که دوست دارید یک اپ یا اپ جدید بسازید، حتما به این موضوع فکر کنید که چرا باید از این به بعد در وبسایت خود استفاده کنید؟ به طور کلی، برای اینکه شما نیازی به یادگیری زبان جاوا یا React یا فریمورک یا Angular و یا هر چیز دیگری که دارید، لازم نیست. اما به خاطر داشته باشید که حتما باید بدونید که چطوری و الان باید اون رو در اون کار کنیم. من یه سری بزنید. حالا باید گفت: خب! شما به سادگی و اینکه چطور؟ خب، من بهتون اجازه بدید! من از کجا و این سوال دارم که چی دارید؟ شما با ما به هیچ وقت اینه که توی این چیه؟ لطفا! خب؟ ما یه برنامه تونستید که الان بهتون کمک کننده یا من اینه؟ تو این رو چطور میتونم بگم چی بگم؟ الان که شما چطوری باید چکار کنم؟ اصلا. ما باید چیکار کنیم؟ خوب نیست! برای چیه، ما الان و …. خب. لطفا. الان چکار کنیم! حالا چطوری برم سراغ چه کسی که تو همین الان، الان! الان چیکار کردم؟ بریم؟ برو سراغتون رو توی همین حالا! ما بهتون گفتم. ولی الان … شما چی گفتم؟ یکم؟ حالا شما یه چیز دیگه، برو دنبال چی دارم؟ این دیگه چی میشه. اینجا؟\n"
          ]
        }
      ],
      "source": [
        "print(\"Generated response: {}\".format(resp))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {
        "id": "coXukIUiJoFB",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 553
        },
        "outputId": "b3b46d62-2f34-45f1-a0a6-f5f2ab03ecfc"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "All model checkpoint layers were used when initializing TFGPT2LMHeadModel.\n",
            "\n",
            "All the layers of TFGPT2LMHeadModel were initialized from the model checkpoint at HooshvareLab/gpt2-fa.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFGPT2LMHeadModel for predictions without further training.\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "ResourceExhaustedError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mResourceExhaustedError\u001b[0m                    Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-35-8dafe3192402>\u001b[0m in \u001b[0;36m<cell line: 57>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     55\u001b[0m \u001b[0mdecoder_input_embeddings\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgpt2_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;34m'input_ids'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mdecoder_inputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'attention_mask'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mdecoder_attention_mask\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     56\u001b[0m \u001b[0mdecoder_input_embeddings\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDropout\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0.1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdecoder_input_embeddings\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 57\u001b[0;31m \u001b[0mdecoder_output\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDense\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgpt2_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvocab_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mactivation\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'softmax'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdecoder_input_embeddings\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     58\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     59\u001b[0m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mModel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mencoder_inputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencoder_attention_mask\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdecoder_inputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdecoder_attention_mask\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdecoder_output\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/keras/utils/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     68\u001b[0m             \u001b[0;31m# To get the full stack trace, call:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m             \u001b[0;31m# `tf.debugging.disable_traceback_filtering()`\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 70\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwith_traceback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfiltered_tb\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     71\u001b[0m         \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     72\u001b[0m             \u001b[0;32mdel\u001b[0m \u001b[0mfiltered_tb\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/keras/backend.py\u001b[0m in \u001b[0;36mrandom_uniform\u001b[0;34m(self, shape, minval, maxval, dtype, nonce)\u001b[0m\n\u001b[1;32m   2099\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mnonce\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2100\u001b[0m                 \u001b[0mseed\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexperimental\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstateless_fold_in\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mseed\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnonce\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2101\u001b[0;31m             return tf.random.stateless_uniform(\n\u001b[0m\u001b[1;32m   2102\u001b[0m                 \u001b[0mshape\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2103\u001b[0m                 \u001b[0mminval\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mminval\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mResourceExhaustedError\u001b[0m: {{function_node __wrapped__Mul_device_/job:localhost/replica:0/task:0/device:GPU:0}} failed to allocate memory [Op:Mul]"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from transformers import AutoTokenizer, TFGPT2LMHeadModel\n",
        "\n",
        "model_name = \"HooshvareLab/gpt2-fa\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "gpt2_model = TFGPT2LMHeadModel.from_pretrained(model_name)\n",
        "\n",
        "df = pd.read_csv('percQA_CleanData.csv', encoding='utf-8')\n",
        "\n",
        "questions = df['question'].tolist()\n",
        "answers = df['answer'].tolist()\n",
        "\n",
        "max_input_length = 512\n",
        "max_target_length = 128\n",
        "\n",
        "tokenized_questions = [tokenizer.encode(q, add_special_tokens=True) for q in questions]\n",
        "tokenized_answers = [tokenizer.encode(a, add_special_tokens=True) for a in answers]\n",
        "\n",
        "encoder_input_ids = tf.keras.preprocessing.sequence.pad_sequences(tokenized_questions, maxlen=max_input_length, dtype='int32', padding='post', truncating='post')\n",
        "encoder_attention_mask = tf.cast(tf.math.not_equal(encoder_input_ids, 0), dtype=tf.int32)\n",
        "decoder_input_ids = tf.keras.preprocessing.sequence.pad_sequences(tokenized_answers, maxlen=max_target_length, dtype='int32', padding='post', truncating='post')[:, :-1]\n",
        "decoder_target_ids = tf.keras.preprocessing.sequence.pad_sequences(tokenized_answers, maxlen=max_target_length, dtype='int32', padding='post', truncating='post')[:, 1:]\n",
        "decoder_attention_mask = tf.cast(tf.math.not_equal(decoder_input_ids, 0), dtype=tf.int32)\n",
        "\n",
        "class ConversationDataset(tf.keras.utils.Sequence):\n",
        "    def __init__(self, encoder_input_ids, encoder_attention_mask, decoder_input_ids, decoder_attention_mask, decoder_target_ids, batch_size):\n",
        "        self.encoder_input_ids = encoder_input_ids\n",
        "        self.encoder_attention_mask = encoder_attention_mask\n",
        "        self.decoder_input_ids = decoder_input_ids\n",
        "        self.decoder_attention_mask = decoder_attention_mask\n",
        "        self.decoder_target_ids = decoder_target_ids\n",
        "        self.batch_size = batch_size\n",
        "\n",
        "    def __len__(self):\n",
        "        return int(np.ceil(len(self.encoder_input_ids) / float(self.batch_size)))\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        batch_encoder_input_ids = self.encoder_input_ids[idx * self.batch_size:(idx + 1) * self.batch_size]\n",
        "        batch_encoder_attention_mask = self.encoder_attention_mask[idx * self.batch_size:(idx + 1) * self.batch_size]\n",
        "        batch_decoder_input_ids = self.decoder_input_ids[idx * self.batch_size:(idx + 1) * self.batch_size]\n",
        "        batch_decoder_attention_mask = self.decoder_attention_mask[idx * self.batch_size:(idx + 1) * self.batch_size]\n",
        "        batch_decoder_target_ids = self.decoder_target_ids[idx * self.batch_size:(idx + 1) * self.batch_size]\n",
        "        return {'encoder_inputs': batch_encoder_input_ids, 'encoder_attention_mask': batch_encoder_attention_mask, 'decoder_inputs': batch_decoder_input_ids, 'decoder_attention_mask': batch_decoder_attention_mask}, batch_decoder_target_ids\n",
        "\n",
        "batch_size = 2\n",
        "dataset = ConversationDataset(encoder_input_ids, encoder_attention_mask, decoder_input_ids, decoder_attention_mask, decoder_target_ids, batch_size)\n",
        "\n",
        "encoder_inputs = tf.keras.layers.Input(shape=(max_input_length,), dtype=tf.int32, name='encoder_inputs')\n",
        "encoder_attention_mask = tf.keras.layers.Input(shape=(max_input_length,), dtype=tf.int32, name='encoder_attention_mask')\n",
        "encoder_output = gpt2_model({'input_ids': encoder_inputs, 'attention_mask': encoder_attention_mask})[0]\n",
        "decoder_inputs = tf.keras.layers.Input(shape=(max_target_length,), dtype=tf.int32, name='decoder_inputs')\n",
        "decoder_attention_mask = tf.keras.layers.Input(shape=(max_target_length,), dtype=tf.int32, name='decoder_attention_mask')\n",
        "decoder_input_embeddings = gpt2_model({'input_ids': decoder_inputs, 'attention_mask': decoder_attention_mask})[0]\n",
        "decoder_input_embeddings = tf.keras.layers.Dropout(0.1)(decoder_input_embeddings)\n",
        "decoder_output = tf.keras.layers.Dense(gpt2_model.config.vocab_size, activation='softmax')(decoder_input_embeddings)\n",
        "\n",
        "model = tf.keras.models.Model(inputs=[encoder_inputs, encoder_attention_mask, decoder_inputs, decoder_attention_mask], outputs=decoder_output)\n",
        "\n",
        "model.summary()\n",
        "\n",
        "optimizer = tf.keras.optimizers.Adam(learning_rate=5e-5)\n",
        "model.compile(optimizer=optimizer, loss='sparse_categorical_crossentropy')\n",
        "\n",
        "epochs = 5\n",
        "model.fit(dataset, epochs=epochs)\n",
        "\n",
        "\n",
        "questions = 'سلام '\n",
        "max_output_length = 128\n",
        "\n",
        "tokenized_questions = [tokenizer.encode(q, add_special_tokens=True) for q in questions]\n",
        "encoder_input_ids = tf.keras.preprocessing.sequence.pad_sequences(tokenized_questions, maxlen=max_input_length, dtype='int32', padding='post', truncating='post')\n",
        "encoder_attention_mask = tf.cast(tf.math.not_equal(encoder_input_ids, 0), dtype=tf.int32)\n",
        "decoder_input_ids = np.zeros((len(questions), max_output_length), dtype='int32')\n",
        "decoder_input_ids[:, 0] = tokenizer.cls_token_id\n",
        "\n",
        "for i in range(1, max_output_length):\n",
        "    decoder_attention_mask = tf.cast(tf.math.not_equal(decoder_input_ids, 0), dtype=tf.int32)\n",
        "    predictions = model([encoder_input_ids, encoder_attention_mask, decoder_input_ids, decoder_attention_mask])\n",
        "    predictions = predictions[:, i-1, :]\n",
        "    predicted_id = tf.argmax(predictions, axis=-1).numpy()\n",
        "    decoder_input_ids[:, i] = predicted_id\n",
        "    if np.all(predicted_id == tokenizer.sep_token_id):\n",
        "        break\n",
        "\n",
        "responses = [tokenizer.decode(output, skip_special_tokens=True) for output in decoder_input_ids]\n",
        "responses"
      ]
    }
  ],
  "metadata": {
    "accelerator": "TPU",
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}