{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/MaryamNourii/ChatBot/blob/main/Basic_Fa_GPT2_chatbot.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-_jjlVOK8pmW"
      },
      "outputs": [],
      "source": [
        "!pip install tensorflow\n",
        "!pip install transformers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1_8tfQ758kgw"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "from transformers import AutoTokenizer, TFAutoModelForCausalLM\n",
        "\n",
        "model_name = \"HooshvareLab/gpt2-fa\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "model = TFAutoModelForCausalLM.from_pretrained(model_name)\n",
        "\n",
        "def generate_response(input_text, max_length=500):\n",
        "    encoded_input = tokenizer(input_text, return_tensors='tf')\n",
        "\n",
        "    generated_output = model.generate(\n",
        "        input_ids=encoded_input['input_ids'],\n",
        "        attention_mask=encoded_input['attention_mask'],\n",
        "        max_length=max_length,\n",
        "        num_beams=5,\n",
        "        no_repeat_ngram_size=2,\n",
        "        early_stopping=True\n",
        "    )\n",
        "\n",
        "    generated_text = tokenizer.decode(generated_output[0], skip_special_tokens=True)\n",
        "\n",
        "    return generated_text.strip()\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "usySoAPj9wCp"
      },
      "outputs": [],
      "source": [
        "# input_text = 'سلام چطوری؟ من نمی دونم چیکار کنم با این درسا، لطفا زاهنماییم کن'\n",
        "input_text = 'سلام چطوری؟ من چیکار کنم؟'\n",
        "\n",
        "resp = generate_response(input_text)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KmGyK5gY-orZ",
        "outputId": "694dbe5b-f697-4148-e64e-3834e6afbacb"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Generated response: سلام چطوری؟ من نمی دونم چیکار کنم با این درسا، لطفا زاهنماییم کنارتون رو عوض کنید. خب حالا بریم سراغ اصل مطلب. اول از همه باید بگم که من به عنوان یک برنامه نویس حرفه‌ای، همیشه سعی می‌کنم خودم رو به روز نگه دارم و همیشه در حال یادگیری باشم. به همین دلیل هم هست که خیلی از برنامه نویسان تازه کار، به دنبال یادگیری مهارت‌های برنامه نویسی هستند. در این مقاله قصد دارم به شما یاد بدم که چطور با استفاده از زبان‌ها و فریم ورک هایی مثل React، Angular، Vue، React و … به راحتی و با صرف کمترین زمان، برنامه‌ی خود را بسازید. پس با من همراه باشید. برای شروع، شما نیاز دارید که یک وب سایت یا یک اپلیکیشن را ایجاد کنید و سپس آن را در مرورگر خود باز کرده و شروع به ساخت یک وبسایت یا اپلیکیشن جدید بکنید. این کار بسیار ساده است، فقط کافی است که به سایت خود بروید و در آن ثبت نام کنید، سپس یک حساب کاربری بسازید و وارد آن شوید. بعد از ساخت وبسایت، باید به سراغ ساخت اپلیکیشن بروید. شما باید یک اکانت بسازید تا بتوانید از طریق آن، اپلیکیشن یا وب سایتی که ساخته‌اید را به صورت رایگان در اختیار دیگران قرار دهید. اگر شما هم از آن دسته افرادی هستید که دوست دارید یک اپ یا اپ جدید بسازید، حتما به این موضوع فکر کنید که چرا باید از این به بعد در وبسایت خود استفاده کنید؟ به طور کلی، برای اینکه شما نیازی به یادگیری زبان جاوا یا React یا فریمورک یا Angular و یا هر چیز دیگری که دارید، لازم نیست. اما به خاطر داشته باشید که حتما باید بدونید که چطوری و الان باید اون رو در اون کار کنیم. من یه سری بزنید. حالا باید گفت: خب! شما به سادگی و اینکه چطور؟ خب، من بهتون اجازه بدید! من از کجا و این سوال دارم که چی دارید؟ شما با ما به هیچ وقت اینه که توی این چیه؟ لطفا! خب؟ ما یه برنامه تونستید که الان بهتون کمک کننده یا من اینه؟ تو این رو چطور میتونم بگم چی بگم؟ الان که شما چطوری باید چکار کنم؟ اصلا. ما باید چیکار کنیم؟ خوب نیست! برای چیه، ما الان و …. خب. لطفا. الان چکار کنیم! حالا چطوری برم سراغ چه کسی که تو همین الان، الان! الان چیکار کردم؟ بریم؟ برو سراغتون رو توی همین حالا! ما بهتون گفتم. ولی الان … شما چی گفتم؟ یکم؟ حالا شما یه چیز دیگه، برو دنبال چی دارم؟ این دیگه چی میشه. اینجا؟\n"
          ]
        }
      ],
      "source": [
        "print(\"Generated response: {}\".format(resp))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "coXukIUiJoFB",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000,
          "referenced_widgets": [
            "09986e7f247346e69ce73cb8a12e1899",
            "3bab89aee8a24a689c2958818968b35d",
            "940b73435c5447c3839030d5daf374b8",
            "f38301d3995448e8a3054fad18214100",
            "0eb7e10c6f7e4a34920f02d1c4bf3d63",
            "062866c401674902b39ffa49f928acb8",
            "e47eb4c7eaf449e6b5e991ba197ea3fc",
            "f87d3dd0ba484d349fd659e63a48daf4",
            "4c489776433f4b83b4a065c27a38adbb",
            "c5f48d9559a24b9398dc62b208557225",
            "170176c9c8aa400a9a1d8d30dfd266aa"
          ]
        },
        "outputId": "7028a0b3-23ed-48e7-b0d6-108a05da6899"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Downloading (…)/main/tokenizer.json:   0%|          | 0.00/2.75M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "09986e7f247346e69ce73cb8a12e1899"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "You are using a model of type gpt2 to instantiate a model of type bert. This is not supported for all configurations of models and can yield errors.\n",
            "Some layers from the model checkpoint at HooshvareLab/gpt2-fa were not used when initializing TFBertModel: ['transformer']\n",
            "- This IS expected if you are initializing TFBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing TFBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some layers of TFBertModel were not initialized from the model checkpoint at HooshvareLab/gpt2-fa and are newly initialized: ['bert']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"model_1\"\n",
            "__________________________________________________________________________________________________\n",
            " Layer (type)                   Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            " decoder_attention_mask (InputL  [(None, 128)]       0           []                               \n",
            " ayer)                                                                                            \n",
            "                                                                                                  \n",
            " decoder_inputs (InputLayer)    [(None, 128)]        0           []                               \n",
            "                                                                                                  \n",
            " tf_bert_model (TFBertModel)    multiple             118298112   ['decoder_attention_mask[0][0]', \n",
            "                                                                  'decoder_inputs[0][0]']         \n",
            "                                                                                                  \n",
            " dropout_111 (Dropout)          (None, 128, 768)     0           ['tf_bert_model[1][0]']          \n",
            "                                                                                                  \n",
            " encoder_inputs (InputLayer)    [(None, 512)]        0           []                               \n",
            "                                                                                                  \n",
            " encoder_attention_mask (InputL  [(None, 512)]       0           []                               \n",
            " ayer)                                                                                            \n",
            "                                                                                                  \n",
            " dense (Dense)                  (None, 128, 42001)   32298769    ['dropout_111[0][0]']            \n",
            "                                                                                                  \n",
            "==================================================================================================\n",
            "Total params: 150,596,881\n",
            "Trainable params: 150,596,881\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n",
            "Epoch 1/5\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:Gradients do not exist for variables ['tf_bert_model/bert/pooler/dense/kernel:0', 'tf_bert_model/bert/pooler/dense/bias:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss` argument?\n",
            "WARNING:tensorflow:Gradients do not exist for variables ['tf_bert_model/bert/pooler/dense/kernel:0', 'tf_bert_model/bert/pooler/dense/bias:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss` argument?\n",
            "WARNING:tensorflow:Gradients do not exist for variables ['tf_bert_model/bert/pooler/dense/kernel:0', 'tf_bert_model/bert/pooler/dense/bias:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss` argument?\n",
            "WARNING:tensorflow:Gradients do not exist for variables ['tf_bert_model/bert/pooler/dense/kernel:0', 'tf_bert_model/bert/pooler/dense/bias:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss` argument?\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "   81/10863 [..............................] - ETA: 20:32:57 - loss: 4.4808"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-10-05d5ac114ca7>\u001b[0m in \u001b[0;36m<cell line: 66>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     64\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m \u001b[0mepochs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m5\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 66\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mepochs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/keras/utils/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     63\u001b[0m         \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     64\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 65\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     66\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     67\u001b[0m             \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1683\u001b[0m                         ):\n\u001b[1;32m   1684\u001b[0m                             \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_train_batch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1685\u001b[0;31m                             \u001b[0mtmp_logs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1686\u001b[0m                             \u001b[0;32mif\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshould_sync\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1687\u001b[0m                                 \u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masync_wait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/util/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    148\u001b[0m     \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    149\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 150\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    151\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    152\u001b[0m       \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/eager/polymorphic_function/polymorphic_function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    892\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    893\u001b[0m       \u001b[0;32mwith\u001b[0m \u001b[0mOptionalXlaContext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jit_compile\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 894\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    895\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    896\u001b[0m       \u001b[0mnew_tracing_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexperimental_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/eager/polymorphic_function/polymorphic_function.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    924\u001b[0m       \u001b[0;31m# In this case we have created variables on the first call, so we run the\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    925\u001b[0m       \u001b[0;31m# defunned version which is guaranteed to never create variables.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 926\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_no_variable_creation_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=not-callable\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    927\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_variable_creation_fn\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    928\u001b[0m       \u001b[0;31m# Release the lock early so that multiple threads can perform the call\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/eager/polymorphic_function/tracing_compiler.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    141\u001b[0m       (concrete_function,\n\u001b[1;32m    142\u001b[0m        filtered_flat_args) = self._maybe_define_function(args, kwargs)\n\u001b[0;32m--> 143\u001b[0;31m     return concrete_function._call_flat(\n\u001b[0m\u001b[1;32m    144\u001b[0m         filtered_flat_args, captured_inputs=concrete_function.captured_inputs)  # pylint: disable=protected-access\n\u001b[1;32m    145\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/eager/polymorphic_function/monomorphic_function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[0;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[1;32m   1755\u001b[0m         and executing_eagerly):\n\u001b[1;32m   1756\u001b[0m       \u001b[0;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1757\u001b[0;31m       return self._build_call_outputs(self._inference_function.call(\n\u001b[0m\u001b[1;32m   1758\u001b[0m           ctx, args, cancellation_manager=cancellation_manager))\n\u001b[1;32m   1759\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/eager/polymorphic_function/monomorphic_function.py\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[1;32m    379\u001b[0m       \u001b[0;32mwith\u001b[0m \u001b[0m_InterpolateFunctionError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    380\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mcancellation_manager\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 381\u001b[0;31m           outputs = execute.execute(\n\u001b[0m\u001b[1;32m    382\u001b[0m               \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msignature\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    383\u001b[0m               \u001b[0mnum_outputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_num_outputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/eager/execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     50\u001b[0m   \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m     \u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 52\u001b[0;31m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[0m\u001b[1;32m     53\u001b[0m                                         inputs, attrs, num_outputs)\n\u001b[1;32m     54\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from transformers import AutoTokenizer, TFBertModel\n",
        "\n",
        "model_name = \"HooshvareLab/gpt2-fa\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "bert_model = TFBertModel.from_pretrained(model_name)\n",
        "\n",
        "df = pd.read_csv('percQA_CleanData.csv', encoding='utf-8')\n",
        "\n",
        "questions = df['question'].tolist()\n",
        "answers = df['answer'].tolist()\n",
        "\n",
        "max_input_length = 512\n",
        "max_target_length = 128\n",
        "\n",
        "tokenized_questions = [tokenizer.encode(q, add_special_tokens=True) for q in questions]\n",
        "tokenized_answers = [tokenizer.encode(a, add_special_tokens=True) for a in answers]\n",
        "\n",
        "encoder_input_ids = tf.keras.preprocessing.sequence.pad_sequences(tokenized_questions, maxlen=max_input_length, dtype='int32', padding='post', truncating='post')\n",
        "decoder_input_ids = tf.keras.preprocessing.sequence.pad_sequences(tokenized_answers, maxlen=max_target_length, dtype='int32', padding='post', truncating='post')[:, :-1]\n",
        "decoder_target_ids = tf.keras.preprocessing.sequence.pad_sequences(tokenized_answers, maxlen=max_target_length, dtype='int32', padding='post', truncating='post')[:, 1:]\n",
        "encoder_attention_mask = tf.cast(tf.math.not_equal(encoder_input_ids, 0), dtype=tf.int32)\n",
        "decoder_attention_mask = tf.cast(tf.math.not_equal(decoder_input_ids, 0), dtype=tf.int32)\n",
        "\n",
        "class ConversationDataset(tf.keras.utils.Sequence):\n",
        "    def __init__(self, encoder_input_ids, encoder_attention_mask, decoder_input_ids, decoder_attention_mask, decoder_target_ids, batch_size):\n",
        "        self.encoder_input_ids = encoder_input_ids\n",
        "        self.encoder_attention_mask = encoder_attention_mask\n",
        "        self.decoder_input_ids = decoder_input_ids\n",
        "        self.decoder_attention_mask = decoder_attention_mask\n",
        "        self.decoder_target_ids = decoder_target_ids\n",
        "        self.batch_size = batch_size\n",
        "\n",
        "    def __len__(self):\n",
        "        return int(np.ceil(len(self.encoder_input_ids) / float(self.batch_size)))\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        batch_encoder_input_ids = self.encoder_input_ids[idx * self.batch_size:(idx + 1) * self.batch_size]\n",
        "        batch_encoder_attention_mask = self.encoder_attention_mask[idx * self.batch_size:(idx + 1) * self.batch_size]\n",
        "        batch_decoder_input_ids = self.decoder_input_ids[idx * self.batch_size:(idx + 1) * self.batch_size]\n",
        "        batch_decoder_attention_mask = self.decoder_attention_mask[idx * self.batch_size:(idx + 1) * self.batch_size]\n",
        "        batch_decoder_target_ids = self.decoder_target_ids[idx * self.batch_size:(idx + 1) * self.batch_size]\n",
        "        return {'encoder_inputs': batch_encoder_input_ids, 'encoder_attention_mask': batch_encoder_attention_mask, 'decoder_inputs': batch_decoder_input_ids, 'decoder_attention_mask': batch_decoder_attention_mask}, batch_decoder_target_ids\n",
        "\n",
        "batch_size = 2\n",
        "dataset = ConversationDataset(encoder_input_ids, encoder_attention_mask, decoder_input_ids, decoder_attention_mask, decoder_target_ids, batch_size)\n",
        "\n",
        "encoder_inputs = tf.keras.layers.Input(shape=(max_input_length,), dtype=tf.int32, name='encoder_inputs')\n",
        "encoder_attention_mask = tf.keras.layers.Input(shape=(max_input_length,), dtype=tf.int32, name='encoder_attention_mask')\n",
        "encoder_output = bert_model({'input_ids': encoder_inputs, 'attention_mask': encoder_attention_mask})[0]\n",
        "decoder_inputs = tf.keras.layers.Input(shape=(max_target_length,), dtype=tf.int32, name='decoder_inputs')\n",
        "decoder_attention_mask = tf.keras.layers.Input(shape=(max_target_length,), dtype=tf.int32, name='decoder_attention_mask')\n",
        "decoder_input_embeddings = bert_model({'input_ids': decoder_inputs, 'attention_mask': decoder_attention_mask})[0]\n",
        "decoder_input_embeddings = tf.keras.layers.Dropout(0.1)(decoder_input_embeddings)\n",
        "decoder_output = tf.keras.layers.Dense(bert_model.config.vocab_size, activation='softmax')(decoder_input_embeddings)\n",
        "\n",
        "model = tf.keras.models.Model(inputs=[encoder_inputs, encoder_attention_mask, decoder_inputs, decoder_attention_mask], outputs=decoder_output)\n",
        "model.summary()\n",
        "\n",
        "optimizer = tf.keras.optimizers.Adam(learning_rate=5e-5)\n",
        "model.compile(optimizer=optimizer, loss='sparse_categorical_crossentropy')\n",
        "\n",
        "epochs = 5\n",
        "model.fit(dataset, epochs=epochs)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_response(input_text, context=None):\n",
        "    max_response_length = 128\n",
        "    input_ids = tokenizer.encode(input_text, add_special_tokens=True, max_length=max_input_length, truncation=True, padding='max_length', return_tensors='tf')\n",
        "    decoder_input_ids = tf.expand_dims([tokenizer.cls_token_id], 0)\n",
        "    decoder_attention_mask = tf.ones_like(decoder_input_ids)\n",
        "    for i in range(max_response_length):\n",
        "        encoder_attention_mask = tf.ones_like(input_ids)\n",
        "        outputs = model([input_ids, encoder_attention_mask, decoder_input_ids, decoder_attention_mask])\n",
        "        logits = outputs[:, -1, :]\n",
        "        predicted_id = tf.argmax(logits, axis=-1)\n",
        "        if predicted_id == tokenizer.sep_token_id:\n",
        "            break\n",
        "        decoder_input_ids = tf.concat([decoder_input_ids, predicted_id], axis=-1)\n",
        "        decoder_attention_mask = tf.concat([decoder_attention_mask, tf.ones_like(predicted_id)], axis=-1)\n",
        "    response = tokenizer.decode(decoder_input_ids[0], skip_special_tokens=True)\n",
        "    return response"
      ],
      "metadata": {
        "id": "HBOnm-vzRRdg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# input_text = 'سلام چطوری؟ من نمی دونم چیکار کنم با این درسا، لطفا زاهنماییم کن'\n",
        "input_text = 'سلام چطوری؟ من چیکار کنم؟'\n",
        "\n",
        "resp = generate_answer(input_text)\n",
        "resp"
      ],
      "metadata": {
        "id": "MDkuYEX_PlJc"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "09986e7f247346e69ce73cb8a12e1899": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_3bab89aee8a24a689c2958818968b35d",
              "IPY_MODEL_940b73435c5447c3839030d5daf374b8",
              "IPY_MODEL_f38301d3995448e8a3054fad18214100"
            ],
            "layout": "IPY_MODEL_0eb7e10c6f7e4a34920f02d1c4bf3d63"
          }
        },
        "3bab89aee8a24a689c2958818968b35d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_062866c401674902b39ffa49f928acb8",
            "placeholder": "​",
            "style": "IPY_MODEL_e47eb4c7eaf449e6b5e991ba197ea3fc",
            "value": "Downloading (…)/main/tokenizer.json: 100%"
          }
        },
        "940b73435c5447c3839030d5daf374b8": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_f87d3dd0ba484d349fd659e63a48daf4",
            "max": 2748949,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_4c489776433f4b83b4a065c27a38adbb",
            "value": 2748949
          }
        },
        "f38301d3995448e8a3054fad18214100": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_c5f48d9559a24b9398dc62b208557225",
            "placeholder": "​",
            "style": "IPY_MODEL_170176c9c8aa400a9a1d8d30dfd266aa",
            "value": " 2.75M/2.75M [00:00&lt;00:00, 14.1MB/s]"
          }
        },
        "0eb7e10c6f7e4a34920f02d1c4bf3d63": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "062866c401674902b39ffa49f928acb8": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "e47eb4c7eaf449e6b5e991ba197ea3fc": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "f87d3dd0ba484d349fd659e63a48daf4": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "4c489776433f4b83b4a065c27a38adbb": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "c5f48d9559a24b9398dc62b208557225": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "170176c9c8aa400a9a1d8d30dfd266aa": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}